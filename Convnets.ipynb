{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXSuKlwKSzEF9NU/OyL9Is",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bohdan-at-Kulinich/Machine-Learning/blob/main/Convnets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction to Convolutional Neural Networks"
      ],
      "metadata": {
        "id": "cR5smyrZNzb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating a small convnet: \n",
        "# using the Functional API\n",
        "\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras import layers \n",
        "\n",
        "# a convnet takes as input tensors of shape (image_height, image_width, image_channels) not including the batch dimension: \n",
        "# (28, 28, 1) is the format of MNIST images. \n",
        "inputs = keras.Input(shape=(28, 28, 1)) \n",
        "\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation='relu')(inputs) \n",
        "x = layers.MaxPooling2D(pool_size=2)(x) \n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation='relu')(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x) \n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation='relu')(x) \n",
        "x = layers.Flatten()(x) \n",
        "\n",
        "outputs = layers.Dense(10, activation='softmax')(x) \n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs) "
      ],
      "metadata": {
        "id": "XeM7EC7PN8A-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the model's summary: \n",
        "\n",
        "model.summary() \n",
        "\n",
        "# The output of each Conv2D and MaxPooling2D layer is a rank-3 tensor (height, width, channels). \n",
        "# The heigth and width dimensions tend to shrink as we go deeper into the model. \n",
        "# The number of channels is cotrolled by the first argument passed to the Conv2D layers (32, 64, 128). \n",
        "\n",
        "# After the last Conv2D layer we end up with an output of shape (3, 3, 128): \n",
        "# a 3x3 feature map of 128 channels. \n",
        "# In the next step we need to feed this rank-3 output into the a densely connected calssifier:\n",
        "# a stack of Dense layers, which process 1D vectors. \n",
        "# We need to flatten the 3D outputs to 1D with a Flatten layer befor adding the Dense layers. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyWi47LgPNdS",
        "outputId": "2f3b1edf-4e7d-41c8-f38f-a1db9ddf33eb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 13, 13, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 3, 3, 128)         73856     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1152)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                11530     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 104,202\n",
            "Trainable params: 104,202\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the convnet on MNIST dataset: \n",
        "\n",
        "# Since we are doing 10-way classification with a softmax output, \n",
        "# we use categorical crossentropy loss, \n",
        "# and because our labels are integers, \n",
        "# we use the sparse version, sparse_categorical_crossentropy: \n",
        "\n",
        "from tensorflow.keras.datasets import mnist \n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data() \n",
        "\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype('float32') / 255 \n",
        "\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype('float32') / 255 \n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train_images, train_labels,\n",
        "          epochs=5, \n",
        "          batch_size=64) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWu8ZzVvP71u",
        "outputId": "b2ee7dea-833c-4e18-83ee-222b415151e8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "938/938 [==============================] - 36s 38ms/step - loss: 0.1587 - accuracy: 0.9501\n",
            "Epoch 2/5\n",
            "938/938 [==============================] - 36s 39ms/step - loss: 0.0452 - accuracy: 0.9862\n",
            "Epoch 3/5\n",
            "938/938 [==============================] - 35s 38ms/step - loss: 0.0315 - accuracy: 0.9904\n",
            "Epoch 4/5\n",
            "938/938 [==============================] - 36s 38ms/step - loss: 0.0229 - accuracy: 0.9927\n",
            "Epoch 5/5\n",
            "938/938 [==============================] - 36s 38ms/step - loss: 0.0186 - accuracy: 0.9940\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7f5e21ead0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the convnet: \n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels) \n",
        "print(f\"Test accuracy: {test_acc:.3f}\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrmhL5V6UjJ_",
        "outputId": "ef8280fc-79b8-478f-e6c9-db408ab05f5d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 7ms/step - loss: 0.0376 - accuracy: 0.9881\n",
            "Test accuracy: 0.988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The convolution operation: "
      ],
      "metadata": {
        "id": "wp6O3hbyVsya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dense layers learn global patterns in their input feature space (for MNIST \n",
        "digit, patterns involving app pixels). \n",
        "\n",
        "Convolution layers learn local patterns (for images, patterns found in small\n",
        "2D windows of the inputs): \n",
        "\n",
        "1.   Patterns are translation invariant: a certain pattern learned in the one corner of the image, is recognized by convnet anywhere.\n",
        "2.   Convnets can learn spatial heirarchies of patterns. This allows to learn increasingly complex and abstract visual concepts, based on the previously learned small local patterns (such as edges, elementary lines, and textures). \n",
        "\n",
        "Convolutions operate over rank-3 tensors called *feature maps*, with two spacial axes (height, width) as well as *depth* axis (*channels* axis). For an RGB image, the dimension of the depth axis is 3, since the image has 3 color channels: red, green and blue. For a black-and-white picture, the depth is 1 (levels of gray). \n",
        "\n",
        "The convolution operation pruduces an output feature map which is still an rank-3 tensor. Its depth can be arbitrary, because the output depth is a parameter of the layer. The different channels now stand for *filters*.  \n",
        "\n",
        "Filters encode specific aspects of the input data (like \"presence of a face in the input\" at a high-level). \n",
        "\n"
      ],
      "metadata": {
        "id": "i2aafpRQVw_9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ui2so0XEZHua"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}